from fastapi import FastAPI, HTTPException, Query
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import pandas as pd
import numpy as np
import json
import re
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from typing import Optional, List, Dict, Any
from contextlib import asynccontextmanager
import os
from sklearn.feature_extraction.text import CountVectorizer
import psycopg2
import re

# DB Connection Details - Support both Spring format and legacy format
def parse_spring_datasource_url(url):
    """Parse jdbc:postgresql://host:port/database?params format"""
    if not url:
        return None, None, None
    # Pattern: jdbc:postgresql://host:port/database
    match = re.match(r'jdbc:postgresql://([^:]+):(\d+)/([^?]+)', url)
    if match:
        return match.group(1), match.group(2), match.group(3)
    return None, None, None

# Try Spring format first, fall back to legacy format
SPRING_URL = os.environ.get("SPRING_DATASOURCE_URL", "")
_parsed_host, _parsed_port, _parsed_db = parse_spring_datasource_url(SPRING_URL)

DB_HOST = _parsed_host or os.environ.get("DB_HOST", "db")
DB_PORT = _parsed_port or os.environ.get("DB_PORT", "5432")
DB_NAME = _parsed_db or os.environ.get("POSTGRES_DB", "bigproject")
DB_USER = os.environ.get("SPRING_DATASOURCE_USERNAME") or os.environ.get("POSTGRES_USER", "postgres")
DB_PASS = os.environ.get("SPRING_DATASOURCE_PASSWORD") or os.environ.get("POSTGRES_PASSWORD", "postgres")

def get_db_connection():
    try:
        conn = psycopg2.connect(
            host=DB_HOST,
            dbname=DB_NAME,
            user=DB_USER,
            password=DB_PASS,
            port=DB_PORT,
            sslmode='require'
        )
        return conn
    except Exception as e:
        print(f"DB Connection Failed: {e}")
        return None

# êµ­ê°€ ë§¤í•‘
COUNTRY_MAPPING = {
    'ë¯¸êµ­': 'US',
    'ì¤‘êµ­': 'CN',
    'ì¼ë³¸': 'JP',
    'ë² íŠ¸ë‚¨': 'VN',
    'ë…ì¼': 'DE'
}

REVERSE_MAPPING = {v: k for k, v in COUNTRY_MAPPING.items()} # {'US': 'ë¯¸êµ­', ...}

# UI í‘œì‹œ ì´ë¦„ -> CSV ì €ì¥ ì´ë¦„ ë§¤í•‘
UI_TO_CSV_ITEM_MAPPING = {
    "ê°„ì¥": "ê°„ì¥", "ê°": "ê°", "ê±´ê³ ì‚¬ë¦¬": "ê³ ì‚¬ë¦¬", "ê³ ì¶”ì¥": "ê³ ì¶”ì¥", "êµ­ìˆ˜": "êµ­ìˆ˜",
    "ì°¸ì¹˜ í†µì¡°ë¦¼": "ê¸°ë¦„ì— ë‹´ê·¼ ê²ƒ", "ê¹€ì¹˜": "ê¹€ì¹˜", "ê¹ë§ˆëŠ˜": "ê»ì§ˆì„ ê¹ ê²ƒ", "ê¹€ë°¥ë¥˜": "ëƒ‰ë™ê¹€ë°¥",
    "ëƒ‰ë©´": "ëƒ‰ë©´", "ë‹¹ë©´": "ë‹¹ë©´", "ê±´ë”ë•": "ë”ë•", "ëœì¥": "ëœì¥", "ë‘ë¶€": "ë‘ë¶€",
    "ë“¤ê¸°ë¦„": "ë“¤ê¸°ë¦„ê³¼ ê·¸ ë¶„íšë¬¼", "ë¼ë©´": "ë¼ë©´", "ìŒ€": "ë©¥ìŒ€", "í”¼í´ ë° ì ˆì„ì±„ì†Œ": "ë°€íìš©ê¸°ì— ë„£ì€ ê²ƒ",
    "ëƒ‰ë™ ë°¤": "ë°¤", "ì¿ í‚¤ ë° í¬ë˜ì»¤": "ë¹„ìŠ¤í‚·, ì¿ í‚¤ì™€ í¬ë˜ì»¤", "ì‚¼ê³„íƒ•": "ì‚¼ê³„íƒ•", "ì†Œì‹œì§€": "ì†Œì‹œì§€",
    "ì†Œì£¼": "ì†Œì£¼", "ë§Œë‘": "ì†ì„ ì±„ìš´ íŒŒìŠ¤íƒ€(ì¡°ë¦¬í•œ ê²ƒì¸ì§€ ë˜ëŠ” ê·¸ ë°–ì˜ ë°©ë²•ìœ¼ë¡œ ì¡°ì œí•œ ê²ƒì¸ì§€ì— ìƒê´€ì—†ë‹¤)",
    "ì´ˆì½”íŒŒì´ë¥˜": "ìŠ¤ìœ„íŠ¸ ë¹„ìŠ¤í‚·", "ë–¡ë³¶ì´ ë–¡": "ìŒ€ê°€ë£¨ì˜ ê²ƒ", "ì „í†µ í•œê³¼/ì•½ê³¼": "ìŒ€ê³¼ì", "ìœ ì": "ìœ ì",
    "ì¸ìŠ¤í„´íŠ¸ ì»¤í”¼": "ì¸ìŠ¤í„´íŠ¸ ì»¤í”¼ì˜ ì¡°ì œí’ˆ", "ì¦‰ì„ë°¥": "ì°Œê±°ë‚˜ ì‚¶ì€ ìŒ€", "ì°¸ê¸°ë¦„": "ì°¸ê¸°ë¦„ê³¼ ê·¸ ë¶„íšë¬¼",
    "ì¶˜ì¥": "ì¶˜ì¥", "ë§‰ê±¸ë¦¬": "íƒì£¼", "ìŒ€ íŠ€ë°¥": "íŠ€ê¸´ ìŒ€", "íŒ½ì´ë²„ì„¯": "íŒ½ì´ë²„ì„¯", 
    "í‘œê³ ë²„ì„¯": "í‘œê³ ë²„ì„¯", "ìŒˆì¥ ë° ì–‘ë…ì¥": "í˜¼í•©ì¡°ë¯¸ë£Œ", "í™ì‚¼ ì—‘ê¸°ìŠ¤": "í™ì‚¼ ì¶”ì¶œë¬¼(extract)"
}

CSV_TO_UI_ITEM_MAPPING = {v: k for k, v in UI_TO_CSV_ITEM_MAPPING.items()}

# ì•„ì´í…œë³„ ê²€ìƒ‰ì–´(Trend Keyword) ë§¤í•‘
# íŠ¸ë Œë“œ ë°ì´í„° ì»¬ëŸ¼ëª… ì˜ˆì‹œ: {COUNTRY}_{KEYWORD}_mean
ITEM_TO_TREND_MAPPING = {
    "ê°„ì¥": "Gochujang",
    "ê³ ì¶”ì¥": "Gochujang",
    "ëœì¥": "Doenjang",
    "ì¶˜ì¥": "Gochujang",
    "ìŒˆì¥ ë° ì–‘ë…ì¥": "Ssamjang",
    "ê¹€ì¹˜": "Kimchi",
    "ë¼ë©´": "Ramyun",
    "êµ­ìˆ˜": "Ramyun",
    "ëƒ‰ë©´": "Ramyun",
    "ë‹¹ë©´": "Ramyun",
    "ì†Œì£¼": "Soju",
    "ë§‰ê±¸ë¦¬": "Makgeolli",
    "ê¹€ë°¥ë¥˜": "Gimbap",
    "ë–¡ë³¶ì´ ë–¡": "Tteokbokki",
    "ìœ ì": "Yuja",
    "ë§Œë‘": "KFood",
    "ì‚¼ê³„íƒ•": "KFood",
    "ì°¸ì¹˜ í†µì¡°ë¦¼": "KFood",
    "ì´ˆì½”íŒŒì´ë¥˜": "KFood",
    "ì¿ í‚¤ ë° í¬ë˜ì»¤": "KFood",
    "ì „í†µ í•œê³¼/ì•½ê³¼": "KFood",
    "ì¸ìŠ¤í„´íŠ¸ ì»¤í”¼": "KFood",
    "ì¦‰ì„ë°¥": "KFood",
    "ìŒ€": "KFood",
    "ë‘ë¶€": "KFood",
    "ë“¤ê¸°ë¦„": "KFood",
    "ì°¸ê¸°ë¦„": "KFood",
    "íŒ½ì´ë²„ì„¯": "KFood",
    "í‘œê³ ë²„ì„¯": "KFood",
    "í™ì‚¼ ì—‘ê¸°ìŠ¤": "KFood"
}

df = None
growth_summary_df = None
df_consumer = None
GLOBAL_MEAN_SENTIMENT = 0.5
GLOBAL_STD_SENTIMENT = 0.3
GLOBAL_MEAN_RATING = 3.0

# =============================================================================
# í—¬í¼ í•¨ìˆ˜: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ë¶„ì„ ì§€í‘œ ê³„ì‚°
# =============================================================================

def remove_pos_tags(text: str) -> str:
    """cleaned_textì—ì„œ _NOUN, _ADJ, _VERB ë“± í’ˆì‚¬ íƒœê·¸ ì œê±°
    
    Example: 'taste_NOUN good_ADJ' -> 'taste good'
    """
    if not isinstance(text, str):
        return ""
    return re.sub(r'_[A-Z]+', '', text)


def extract_bigrams_with_metrics(
    texts: pd.Series, 
    ratings: pd.Series, 
    original_texts: pd.Series,
    top_n: int = 15,
    adj_priority: bool = True,
    min_df: int = 5
) -> List[Dict[str, Any]]:
    """
    Bigram ì¶”ì¶œ í›„ Impact Score, Positivity Rate ê³„ì‚°.
    í˜•ìš©ì‚¬(_ADJ) í¬í•¨ ì¡°í•©ì„ ìš°ì„ ìˆœìœ„ë¡œ ì œì•ˆ.
    
    Args:
        texts: cleaned_text ì»¬ëŸ¼ (í’ˆì‚¬ íƒœê·¸ í¬í•¨)
        ratings: rating ì»¬ëŸ¼
        original_texts: original_text ì»¬ëŸ¼ (Drill-downìš©)
        top_n: ë°˜í™˜í•  ìƒìœ„ í‚¤ì›Œë“œ ìˆ˜
        adj_priority: í˜•ìš©ì‚¬ í¬í•¨ Bigramë§Œ ë…¸ì¶œí• ì§€ ì—¬ë¶€ (Falseë©´ ëª¨ë“  Bigram ë…¸ì¶œ)
        min_df: CountVectorizerì˜ ìµœì†Œ ë“±ì¥ ë¹ˆë„
    
    Returns:
        List of keyword analysis dicts with impact_score, positivity_rate, sample_reviews
    """
    if texts.empty:
        return []
    
    # 1. Bigram ì¶”ì¶œ
    try:
        # í’ˆì‚¬ íƒœê·¸ ì œê±°ëœ í…ìŠ¤íŠ¸ë¡œ Bigram ì¶”ì¶œ
        cleaned_texts_no_tags = texts.apply(remove_pos_tags).fillna('')
        
        vectorizer = CountVectorizer(
            ngram_range=(2, 2),
            min_df=min_df,
            max_features=500,
            stop_words='english',
            token_pattern=r'\b[a-zA-Z]{3,}\b'
        )
        
        bigram_matrix = vectorizer.fit_transform(cleaned_texts_no_tags)
        bigram_names = vectorizer.get_feature_names_out()
        bigram_counts = bigram_matrix.sum(axis=0).A1
        
    except Exception as e:
        print(f"Bigram ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return []
    
    # 2. í˜•ìš©ì‚¬ í¬í•¨ Bigram í•„í„°ë§ (ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ _ADJ íƒœê·¸ í™•ì¸)
    adj_bigrams = set()
    if adj_priority:
        # ì£¼ì„: textsëŠ” íƒœê·¸ê°€ í¬í•¨ëœ cleaned_text ì»¬ëŸ¼ì„
        try:
            all_text = " ".join(texts.dropna().astype(str))
            for bigram in bigram_names:
                words = bigram.split()
                if len(words) == 2:
                    if f"{words[0]}_ADJ" in all_text or f"{words[1]}_ADJ" in all_text:
                        adj_bigrams.add(bigram)
        except Exception as e:
            print(f"í˜•ìš©ì‚¬ í•„í„°ë§ ì˜¤ë¥˜: {e}")

    # 3. ê° Bigramì— ëŒ€í•´ Impact Score, Positivity Rate ê³„ì‚°
    results = []
    
    for idx, bigram in enumerate(bigram_names):
        count = int(bigram_counts[idx])
        if count < min_df: # min_dfë³´ë‹¤ ì ìœ¼ë©´ pass (CountVectorizerì—ì„œ ì´ë¯¸ ê±¸ëŸ¬ì¡Œê² ì§€ë§Œ ì•ˆì „ì¥ì¹˜)
            continue
            
        # í•´ë‹¹ Bigramì„ í¬í•¨í•˜ëŠ” ë¦¬ë·° í•„í„°ë§
        # cleaned_texts_no_tagsë¥¼ ì‚¬ìš©í•´ì•¼ í•¨
        mask = cleaned_texts_no_tags.str.contains(bigram, case=False, na=False, regex=False)
        matching_ratings = ratings[mask]
        matching_originals = original_texts[mask]
        
        if len(matching_ratings) == 0:
            continue
        
        # Impact Score = í•´ë‹¹ í‚¤ì›Œë“œ í¬í•¨ í‰ê·  - ì „ì²´ í‰ê· (3.0)
        avg_rating = matching_ratings.mean()
        impact_score = round(avg_rating - 3.0, 2)
        
        # Positivity Rate = 4-5ì  ë¹„ìœ¨ (%)
        positive_count = (matching_ratings >= 4).sum()
        positivity_rate = round((positive_count / len(matching_ratings)) * 100, 1)
        
        # Satisfaction Index = (5ì  ë¦¬ë·° ë¹„ìœ¨) / 0.2 (ì „ì²´ 5ì  í™•ë¥ )
        five_star_ratio = (matching_ratings == 5).mean()
        satisfaction_index = round(five_star_ratio / 0.2, 2)
        
        # Sample Reviews (ìµœëŒ€ 3ê°œ)
        sample_reviews = matching_originals.dropna().head(3).tolist()
        
        # í˜•ìš©ì‚¬ í¬í•¨ ì—¬ë¶€
        has_adj = bigram in adj_bigrams
        
        results.append({
            "keyword": bigram,
            "impact_score": impact_score,
            "positivity_rate": positivity_rate, # í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€ (API ì“°ëŠ” ë‹¤ë¥¸ ê³³ì´ ìˆì„ ìˆ˜ ìˆìŒ)
            "satisfaction_index": satisfaction_index,
            "mention_count": count,
            "sample_reviews": sample_reviews,
            "has_adjective": has_adj
        })
    
    # 4. ì •ë ¬: í˜•ìš©ì‚¬ í¬í•¨ ìš°ì„ , ê·¸ ë‹¤ìŒ ì–¸ê¸‰ íšŸìˆ˜
    if adj_priority:
        # í˜•ìš©ì‚¬ í¬í•¨ ì¡°í•©ì´ ìˆë‹¤ë©´ ê·¸ê²ƒë“¤ë§Œ í•„í„°ë§í•´ì„œ ìƒìœ„ê¶Œì— ë°°ì¹˜
        adj_results = [r for r in results if r["has_adjective"]]
        if adj_results:
            results = sorted(adj_results, key=lambda x: -x["mention_count"])
        else:
            results = sorted(results, key=lambda x: -x["mention_count"])
    else:
        results = sorted(results, key=lambda x: -x["mention_count"])
    
    return results[:top_n]


def get_diverging_keywords(keywords_analysis: List[Dict], top_n: int = 10, threshold: float = 0.3) -> Dict[str, List[Dict]]:
    """
    Impact Score ê¸°ì¤€ìœ¼ë¡œ ë¶€ì •/ê¸ì • í‚¤ì›Œë“œ ë¶„ë¦¬
    
    Args:
        keywords_analysis: ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        top_n: ê²°ê³¼ë‹¹ ìµœëŒ€ ê°œìˆ˜
        threshold: í•„í„°ë§í•  Impact Scoreì˜ ì ˆëŒ€ê°’ ë¬¸í„± (ë°ì´í„° ì ìœ¼ë©´ 0.0)
    
    Returns:
        {"negative": [...], "positive": [...]}
    """
    # ë¶€ì • í‚¤ì›Œë“œ: impact_score < -threshold
    negative = sorted(
        [k for k in keywords_analysis if k["impact_score"] < -threshold],
        key=lambda x: x["impact_score"]
    )[:top_n]
    
    # ê¸ì • í‚¤ì›Œë“œ: impact_score > threshold
    positive = sorted(
        [k for k in keywords_analysis if k["impact_score"] > threshold],
        key=lambda x: -x["impact_score"]
    )[:top_n]
    
    return {"negative": negative, "positive": positive}

import threading
import time

def load_data_background():
    global df, growth_summary_df
    global GLOBAL_MEAN_SENTIMENT, GLOBAL_STD_SENTIMENT, GLOBAL_MEAN_RATING

    print("ğŸš€ [Background] Starting Data Loading...", flush=True)
    
    # Retry mechanism: Wait for DB migration if needed (Up to 5 minutes)
    max_retries = 60 
    for i in range(max_retries):
        conn = get_db_connection()
        if conn:
            try:
                # 1. Load Export Trends
                print(f"Loading export_trends from DB (Attempt {i+1}/{max_retries})...", flush=True)
                query = "SELECT * FROM export_trends"
                temp_df = pd.read_sql(query, conn)
                
                if not temp_df.empty:
                    # [Optimization] Do NOT expand JSONB trend_data globally at startup.
                    # This saves memory and CPU. We will extract specific fields on-demand in /analyze.
                    df = temp_df

                    # Ensure trend_data is parsed as dict (if it comes as string)
                    if 'trend_data' in df.columns:
                        # Vectorized parsing if possible, or simple apply (parsing 10MB is fast if not normalizing)
                        def ensure_dict(x):
                            if isinstance(x, dict): return x
                            if isinstance(x, str):
                                try: return json.loads(x)
                                except: return {}
                            return {}
                        df['trend_data'] = df['trend_data'].apply(ensure_dict)

                    # Numeric Cleanups
                    numeric_cols = df.select_dtypes(include=[np.number]).columns
                    df[numeric_cols] = df[numeric_cols].fillna(0)
                    
                    # Growth Matrix Calculation
                    print("Calculating Growth Matrix...", flush=True)
                    summaries = []
                    group_cols = ['country_code', 'item_name']
                    if 'country_code' not in df.columns:
                            group_cols = ['country_name', 'item_name']
                            
                    grouped = df.groupby(group_cols)
                    for name, group in grouped:
                        if len(group) < 24: continue
                        group = group.sort_values('period_str')
                        recent_12 = group.tail(12)
                        prev_12 = group.iloc[-24:-12]
                        
                        weight_col = 'export_weight' if 'export_weight' in group.columns else None
                        if weight_col:
                            w_curr = recent_12[weight_col].sum()
                            w_prev = prev_12[weight_col].sum()
                        else: 
                                w_curr = recent_12['export_value'].sum()
                                w_prev = prev_12['export_value'].sum()
        
                        weight_growth = ((w_curr - w_prev) / w_prev * 100) if w_prev > 0 else 0
                        
                        p_curr = recent_12['unit_price'].mean()
                        p_prev = prev_12['unit_price'].mean()
                        price_growth = ((p_curr - p_prev) / p_prev * 100) if p_prev > 0 else 0
                        
                        total_value = recent_12['export_value'].sum()
                        
                        summaries.append({
                            'country': name[0] if 'country_code' in df.columns else COUNTRY_MAPPING.get(name[0], name[0]),
                            'item_csv_name': name[1],
                            'weight_growth': round(weight_growth, 1),
                            'price_growth': round(price_growth, 1),
                            'total_value': total_value
                        })
                    growth_summary_df = pd.DataFrame(summaries)
                    print("Export Trends Loaded & Matrix Calculated.", flush=True)
                    
                    # 2. Global Consumer Stats (Only if step 1 success)
                    print("Calculating Global Consumer Stats from DB...", flush=True)
                    with conn.cursor() as cur:
                        cur.execute("SELECT AVG(sentiment_score), STDDEV(sentiment_score), AVG(rating) FROM amazon_reviews")
                        row = cur.fetchone()
                        if row and row[0] is not None:
                                GLOBAL_MEAN_SENTIMENT = float(row[0])
                                GLOBAL_STD_SENTIMENT = float(row[1]) if row[1] is not None else 0.3
                                GLOBAL_MEAN_RATING = float(row[2])
                                print(f"Global Stats: Sent={GLOBAL_MEAN_SENTIMENT:.2f}, Std={GLOBAL_STD_SENTIMENT:.2f}, Rating={GLOBAL_MEAN_RATING:.2f}", flush=True)
                        else:
                                print("âš ï¸ amazon_reviews table empty or stats unavailable.", flush=True)
                    
                    conn.close()
                    break # Success, exit retry loop
                    
                else:
                    print(f"âš ï¸ export_trends table is empty. Migration might be in progress... (Attempt {i+1}/{max_retries})", flush=True)
                    conn.close()
                    time.sleep(5) # Wait for migration

            except Exception as e:
                print(f"DB Load Failed (Attempt {i+1}/{max_retries}): {e}", flush=True)
                if conn: conn.close()
                time.sleep(5) # Wait before retry
        else:
            print(f"DB Connection Failed (Attempt {i+1}/{max_retries}). Retrying in 5s...", flush=True)
            time.sleep(5)
    
    if df is None or df.empty: 
        print("âŒ Final: Could not load data after retries. App will run with empty state.", flush=True)
        df = pd.DataFrame()
        growth_summary_df = pd.DataFrame()

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Initialize empty first to prevent errors if requests come in before load
    global df, growth_summary_df
    df = pd.DataFrame()
    growth_summary_df = pd.DataFrame()

    print("ğŸš€ Server Starting... Triggering Background Data Load.", flush=True)
    
    # Start background thread for data loading
    # This prevents blocking the startup, so Readiness Probe can pass immediately.
    loader_thread = threading.Thread(target=load_data_background, daemon=True)
    loader_thread.start()

    yield
    print("Shutting down...", flush=True)

app = FastAPI(title="K-Food Export Analysis Engine", lifespan=lifespan)

app.add_middleware(
    CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]
)

@app.get("/")
async def root():
    return {"message": "K-Food Analysis Engine (Visual Analytics Mode)", "status": "Ready"}

@app.get("/health/data")
async def health_data():
    """Check if data is loaded"""
    return {
        "data_loaded": not (df is None or df.empty),
        "rows": len(df) if df is not None else 0,
        "growth_matrix_rows": len(growth_summary_df) if growth_summary_df is not None else 0,
        "global_stats": {
            "sentiment": GLOBAL_MEAN_SENTIMENT,
            "rating": GLOBAL_MEAN_RATING
        }
    }

@app.get("/items")
async def get_items():
    if df is None or df.empty: return {"items": []}
    try:
        csv_items = df['item_name'].dropna().unique().tolist()
        ui_items = sorted(list(set([CSV_TO_UI_ITEM_MAPPING.get(i, i) for i in csv_items])))
        return {"items": ui_items}
    except: return {"items": []}

@app.get("/analyze")
async def analyze(country: str = Query(...), item: str = Query(...)):
    
    # 1. ë§¤í•‘ ë° ìœ íš¨ì„± ê²€ì‚¬
    country_name = REVERSE_MAPPING.get(country, country) # ì½”ë“œ(US) -> ì´ë¦„(ë¯¸êµ­)
    if country in COUNTRY_MAPPING: # ì…ë ¥ì´ í•œê¸€(ë¯¸êµ­)ì´ë©´ ì½”ë“œë¡œ ë³€í™˜
         country_code = COUNTRY_MAPPING[country]
         country_name = country
    else:
         country_code = country # ì…ë ¥ì´ ì½”ë“œ(US)ë©´ ê·¸ëŒ€ë¡œ
         
    csv_item_name = UI_TO_CSV_ITEM_MAPPING.get(item, item)
    
    # ë°ì´í„° í•„í„°ë§
    filtered = df[
        (df['country_name'] == country_name) & 
        (df['item_name'] == csv_item_name)
    ].copy()
    
    if filtered.empty or (filtered['export_value'].sum() == 0):
        return {"has_data": False}

    # ë‚ ì§œìˆœ ì •ë ¬
    filtered = filtered.sort_values('period_str')

    # ---------------------------------------------------------
    # Chart 1: Trend Stack (ìˆ˜ì¶œì•¡ + í™˜ìœ¨ + GDP/Econ)
    # ---------------------------------------------------------
    rows = 2
    titles = ["ìˆ˜ì¶œì•¡ Trend", f"{country_name} í˜„ì§€ í™˜ìœ¨"]
    if 'gdp_level' in filtered.columns:
        rows = 3
        titles.append(f"{country_name} GDP ì§€í‘œ")
        
    fig_stack = make_subplots(rows=rows, cols=1, shared_xaxes=True, 
                              vertical_spacing=0.1, subplot_titles=titles)
                              
    # Row 1: Export Value (Bar + Color Gradient)
    fig_stack.add_trace(go.Bar(
        x=filtered['period_str'], y=filtered['export_value'], name="ìˆ˜ì¶œì•¡ ($)",
        marker=dict(color=filtered['export_value'], colorscale='Purples')
    ), row=1, col=1)
    
    # Row 2: Exchange Rate (Line with Fill)
    fig_stack.add_trace(go.Scatter(
        x=filtered['period_str'], y=filtered['exchange_rate'], name="í™˜ìœ¨",
        line=dict(color='#f59e0b', width=2), fill='tozeroy', fillcolor='rgba(245, 158, 11, 0.1)'
    ), row=2, col=1)
    
    # Row 3: GDP (if exists)
    if rows == 3:
        fig_stack.add_trace(go.Scatter(
            x=filtered['period_str'], y=filtered['gdp_level'], name="GDP",
            line=dict(color='#10b981', width=2, dash='dot')
        ), row=3, col=1)

    fig_stack.update_layout(
        height=600 if rows == 3 else 450, 
        template="plotly_white", 
        showlegend=False,
        margin=dict(l=40, r=20, t=60, b=40)
    )

    # ---------------------------------------------------------
    # Chart 2: Signal Map (Leading-Lagging)
    # ---------------------------------------------------------
    fig_signal = make_subplots(specs=[[{"secondary_y": True}]])
    
    # ---------------------------------------------------------
    # Chart 2: Signal Map (Leading-Lagging)
    # ---------------------------------------------------------
    fig_signal = make_subplots(specs=[[{"secondary_y": True}]])
    
    # [Optimization] Extract Trend Data on-the-fly for filtered rows
    # filtered['trend_data'] is a Series of dicts
    
    common_trend_key = f"{country_code}_KFood_mean"
    
    # Helper to safely get value from dict
    def get_trend_val(row, key):
        td = row.get('trend_data', {})
        if not isinstance(td, dict): return None
        return td.get(key)

    # 1. ê³µí†µ ì„ í–‰ ì§€í‘œ: ì „ì²´ K-Food ê´€ì‹¬ë„ (Baseline)
    # Check if we have data for this key in the first row (as a sample)
    has_common = False
    first_trend_data = filtered.iloc[0].get('trend_data', {}) if not filtered.empty else {}
    if isinstance(first_trend_data, dict) and common_trend_key in first_trend_data:
        has_common = True
        
    if has_common:
        # Extract series
        y_common = filtered.apply(lambda r: get_trend_val(r, common_trend_key), axis=1)
        fig_signal.add_trace(go.Scatter(
            x=filtered['period_str'], y=y_common, 
            name="K-Food ì „ì²´ ê´€ì‹¬ë„",
            line=dict(color='#fda4af', width=2, dash='dot'), # ì—°í•œ í•‘í¬ ì ì„ 
            opacity=0.6
        ), secondary_y=True)

    # 2. ê°œë³„ ì„ í–‰ ì§€í‘œ: 1:1 ë§¤í•‘ëœ í’ˆëª© ê´€ì‹¬ë„
    trend_kw = ITEM_TO_TREND_MAPPING.get(item)
    # KFoodì™€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€ë¡œ ê·¸ë¦¼
    if trend_kw and trend_kw != "KFood":
        specific_trend_key = f"{country_code}_{trend_kw}_mean"
        
        # Check existence
        has_specific = False
        if isinstance(first_trend_data, dict) and specific_trend_key in first_trend_data:
             has_specific = True
             
        if has_specific:
            y_specific = filtered.apply(lambda r: get_trend_val(r, specific_trend_key), axis=1)
            fig_signal.add_trace(go.Scatter(
                x=filtered['period_str'], y=y_specific, 
                name=f"í’ˆëª© ê´€ì‹¬ë„ ({trend_kw})",
                line=dict(color='#f43f5e', width=4), # ì§„í•œ ì¥ë¯¸ìƒ‰ ì‹¤ì„ 
                mode='lines+markers'
            ), secondary_y=True)
            
    elif not has_common:
        # KFoodë„ ì—†ê³  ë§¤í•‘ë„ ì—†ì„ ë•Œë§Œ ì•„ë¬´ íŠ¸ë Œë“œë‚˜ í•˜ë‚˜ ì°¾ì•„ì„œ í‘œì‹œ (í´ë°±)
        # Find any key ending in _mean inside the first row's trend_data
        fallback_key = None
        if isinstance(first_trend_data, dict):
            for k in first_trend_data.keys():
                if k.startswith(f"{country_code}_") and k.endswith("_mean"):
                    fallback_key = k
                    break
        
        if fallback_key:
            y_fallback = filtered.apply(lambda r: get_trend_val(r, fallback_key), axis=1)
            fig_signal.add_trace(go.Scatter(
                x=filtered['period_str'], y=y_fallback, 
                name="ê´€ì‹¬ë„ (ê´€ë ¨ ë°ì´í„°)",
                line=dict(color='#f43f5e', width=3)
            ), secondary_y=True)

    # 3. í›„í–‰ ì§€í‘œ: ì‹¤ì (ìˆ˜ì¶œì•¡) - Area
    fig_signal.add_trace(go.Scatter(
        x=filtered['period_str'], y=filtered['export_value'], name="ìˆ˜ì¶œ ì‹¤ì  ($)",
        fill='tozeroy', line=dict(color='#6366f1', width=0), opacity=0.3
    ), secondary_y=False)
        
    fig_signal.update_layout(
        title="Signal Map (ê´€ì‹¬ë„ vs ì‹¤ì  ì‹œì°¨ ë¶„ì„)",
        template="plotly_white",
        height=400,
        legend=dict(orientation="h", y=1.1, x=0.5, xanchor='center'),
        margin=dict(l=40, r=40, t=60, b=40)
    )
    fig_signal.update_yaxes(title_text="ìˆ˜ì¶œì•¡ ($)", secondary_y=False, showgrid=False)
    fig_signal.update_yaxes(title_text="ê´€ì‹¬ë„ Index", secondary_y=True, showgrid=False)

    # ---------------------------------------------------------
    # Chart 3: Growth Matrix (Scatter Plot)
    # ---------------------------------------------------------
    country_matrix = growth_summary_df[growth_summary_df['country'] == country_code].copy()
    fig_scatter = go.Figure()
    
    if not country_matrix.empty and not country_matrix[country_matrix['item_csv_name'] == csv_item_name].empty:
        country_matrix['ui_name'] = country_matrix['item_csv_name'].apply(lambda x: CSV_TO_UI_ITEM_MAPPING.get(x, x))
        
        curr = country_matrix[country_matrix['item_csv_name'] == csv_item_name]
        others = country_matrix[country_matrix['item_csv_name'] != csv_item_name]
        
        # Others
        fig_scatter.add_trace(go.Scatter(
            x=others['weight_growth'], y=others['price_growth'],
            mode='markers',
            marker=dict(size=10, color='#94a3b8', opacity=0.4),
            text=others['ui_name'], name="íƒ€ í’ˆëª©",
            hovertemplate="<b>%{text}</b><br>ì–‘ì : %{x}%<br>ì§ˆì : %{y}%"
        ))
        
        # Current
        fig_scatter.add_trace(go.Scatter(
            x=curr['weight_growth'], y=curr['price_growth'],
            mode='markers+text',
            marker=dict(size=25, color='#f43f5e', line=dict(width=2, color='white')),
            text=curr['ui_name'], textposition="top center",
            textfont=dict(size=15, color='#f43f5e', family="Arial Black"),
            name=item,
            hovertemplate="<b>%{text}</b> (í˜„ì¬)<br>ì–‘ì : %{x}%<br>ì§ˆì : %{y}%"
        ))
        
        # Quadrant Lines
        fig_scatter.add_hline(y=0, line_dash="solid", line_color="#e2e8f0")
        fig_scatter.add_vline(x=0, line_dash="solid", line_color="#e2e8f0")
        
        # Annotations (1~4ì‚¬ë¶„ë©´)
        # 1ì‚¬ë¶„ë©´ (ìš°ìƒí–¥): Premium Expansion
        fig_scatter.add_annotation(x=10, y=10, text="Premium (ê³ ë¶€ê°€ê°€ì¹˜ ì„±ì¥)", showarrow=False, font=dict(color="#10b981", size=12), xanchor="left")
        # 4ì‚¬ë¶„ë©´ (ìš°í•˜í–¥): Volume Driven
        fig_scatter.add_annotation(x=10, y=-10, text="Volume (ë°•ë¦¬ë‹¤ë§¤)", showarrow=False, font=dict(color="#3b82f6", size=12), xanchor="left")
        
        fig_scatter.update_layout(
            title="ì„±ì¥ì˜ ì§ˆ (Growth Matrix)",
            xaxis_title="ì–‘ì  ì„±ì¥ (ë¬¼ëŸ‰ ì¦ê°€ìœ¨ %)",
            yaxis_title="ì§ˆì  ì„±ì¥ (ë‹¨ê°€ ì¦ê°€ìœ¨ %)",
            template="plotly_white",
            height=500,
            showlegend=False,
            margin=dict(l=40, r=20, t=60, b=40)
        )
    else:
        # ë°ì´í„°ê°€ ë¶€ì¡±í•´ì„œ ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ê·¸ë¦´ ìˆ˜ ì—†ì„ ë•Œ ë¹ˆ ì°¨íŠ¸
        fig_scatter.update_layout(
            title="ì„±ì¥ì˜ ì§ˆ (ë°ì´í„° ë¶€ì¡±)",
            template="plotly_white", height=500
        )

    return {
        "country": country,
        "country_name": country_name,
        "item": item,
        "has_data": True,
        "charts": {
            "trend_stack": json.loads(fig_stack.to_json()),
            "signal_map": json.loads(fig_signal.to_json()),
            "growth_matrix": json.loads(fig_scatter.to_json())
        }
    }

@app.get("/analyze/consumer")
async def analyze_consumer(item_id: str = Query(None, description="ASIN"), item_name: str = Query(None, description="ì œí’ˆëª…/í‚¤ì›Œë“œ")):
    
    
    conn = get_db_connection()
    if not conn:
         return JSONResponse(status_code=500, content={"has_data": False, "message": "Database Connection Error"})

    try:
        # DBì—ì„œ ì§ì ‘ ì¡°íšŒ (Memory Efficient)
        if item_name:
            query = """
                SELECT * FROM amazon_reviews 
                WHERE title ILIKE %s OR cleaned_text ILIKE %s
            """
            search_pattern = f"%{item_name}%"
            filtered = pd.read_sql(query, conn, params=(search_pattern, search_pattern))
            
        elif item_id:
            query = "SELECT * FROM amazon_reviews WHERE asin = %s"
            filtered = pd.read_sql(query, conn, params=(item_id,))
        else:
            filtered = pd.DataFrame()
            
    except Exception as e:
        print(f"Data Fetch Error: {e}")
        filtered = pd.DataFrame()
    finally:
        conn.close()

    if filtered.empty:
        return {"has_data": False, "message": "í•´ë‹¹ ì¡°ê±´ì˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}

    try:
        # === [ì¤‘ìš”] ë°ì´í„° ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë° ëŒ€ì²´ ë¡œì§ ===
        # í‰ì  ë°ì´í„° ìˆ«ì ë³€í™˜
        if 'rating' in filtered.columns:
            filtered['rating'] = pd.to_numeric(filtered['rating'], errors='coerce').fillna(3.0)

        # 1. ê°ì„± ì ìˆ˜ (sentiment_score ì—†ì„ ê²½ìš° í‰ì  ê¸°ë°˜ ìƒì„±)
        if 'sentiment_score' not in filtered.columns:
            if 'rating' in filtered.columns:
                # 1->0.0, 5->1.0 í˜•íƒœë¡œ ë§¤í•‘
                filtered['sentiment_score'] = (filtered['rating'] - 1) / 4
            else:
                filtered['sentiment_score'] = 0.5
    except Exception as e:
        print(f"í•„í„°ë§/ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return {"has_data": False, "message": f"ì„œë²„ ì˜¤ë¥˜: {str(e)}"}

    # 2. êµ¬ë§¤/ì¶”ì²œ ì˜ë„ (ë°ì´í„° ì—†ì„ ê²½ìš° ê³ í‰ì  ê¸°ë°˜ ì¶”ë¡ )
    if 'repurchase_intent_hybrid' not in filtered.columns:
         filtered['repurchase_intent_hybrid'] = filtered['rating'] >= 4
    if 'recommendation_intent_hybrid' not in filtered.columns:
         filtered['recommendation_intent_hybrid'] = filtered['rating'] >= 4
         
    # 3. í‚¤ì›Œë“œ ì»¬ëŸ¼ (ë°ì´í„° ì—†ì„ ê²½ìš° ë¹ˆ ê°’ ìƒì„±)
    for col in ['review_text_keywords', 'title_keywords', 'flavor_terms', 'price', 'quality_issues_semantic', 'delivery_issues_semantic']:
        if col not in filtered.columns:
            filtered[col] = None
            
    # 4. íŒŒìƒ ë³€ìˆ˜ ì´ˆê¸°í™” (DBì— ì—†ê±°ë‚˜ ê³„ì‚°ë˜ì§€ ì•Šì€ ê²½ìš°)
    required_cols = ['value_perception_hybrid', 'price_sensitive', 'sensory_conflict']
    for col in required_cols:
         if col not in filtered.columns:
             filtered[col] = 0.5 if col == 'value_perception_hybrid' else (0.0 if col == 'price_sensitive' else False)

    total_count = filtered.shape[0]
    
    # =========================================================
    # 2. ì‹œì¥ ê°ì„± ë° ì£¼ìš” ì ìˆ˜ (ìƒëŒ€ì  ì§€í‘œë¡œ ì „ë©´ êµì²´)
    # =========================================================
    try:
        # 1. Impact Score (Rating Lift)
        avg_rating = filtered['rating'].mean()
        if pd.isna(avg_rating): avg_rating = 3.0
        item_impact_score = round(avg_rating - 3.0, 2)
        
        # 2. Relative Sentiment Z-Score
        target_mean_sent = filtered['sentiment_score'].mean()
        if pd.isna(target_mean_sent): target_mean_sent = 0.5
        
        if GLOBAL_STD_SENTIMENT > 0:
            sentiment_z_score = round((target_mean_sent - GLOBAL_MEAN_SENTIMENT) / GLOBAL_STD_SENTIMENT, 2)
        else:
            sentiment_z_score = 0.0
            
        # 3. Satisfaction Index (Likelihood Ratio)
        target_five_star_ratio = (filtered['rating'] == 5).mean()
        if pd.isna(target_five_star_ratio): target_five_star_ratio = 0.0
        satisfaction_index = round(target_five_star_ratio / 0.2, 2)
        
        # ìš”ì•½ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        metrics = {
            "impact_score": item_impact_score,
            "sentiment_z_score": sentiment_z_score,
            "satisfaction_index": satisfaction_index,
            "total_reviews": total_count
        }
    except Exception as e:
        print(f"ë©”íŠ¸ë¦­ ê³„ì‚° ì˜¤ë¥˜: {e}")
        metrics = {
            "impact_score": 0, "sentiment_z_score": 0, "satisfaction_index": 0, "total_reviews": total_count
        }
    
    # =========================================================
    # 3. ìƒì„¸ ë¶„ì„: Bigram ê¸°ë°˜ í‚¤ì›Œë“œ ë¶„ì„ (ê°€ë³€ ì„ê³„ê°’ ì ìš©)
    # =========================================================
    
    # ë°ì´í„° ê·œëª¨ì— ë”°ë¥¸ ê°€ë³€ íŒŒë¼ë¯¸í„° ê²°ì •
    is_small_sample = total_count < 50
    adj_priority_val = not is_small_sample # 50ê°œ ë¯¸ë§Œì´ë©´ False (ëª¨ë“  í‚¤ì›Œë“œ í—ˆìš©)
    min_df_val = 1 if is_small_sample else 2 # 50ê°œ ë¯¸ë§Œì´ë©´ 1ë²ˆë§Œ ë‚˜ì™€ë„ ì¶”ì¶œ
    impact_threshold_val = 0.0 if is_small_sample else 0.3 # 50ê°œ ë¯¸ë§Œì´ë©´ ëª¨ë“  ì°¨ì´ ë…¸ì¶œ
    
    # Bigram ì¶”ì¶œ ë° Impact Score, Positivity Rate ê³„ì‚°
    keywords_analysis = []
    diverging_keywords = {"negative": [], "positive": []}
    
    try:
        if 'cleaned_text' in filtered.columns and 'original_text' in filtered.columns:
            keywords_analysis = extract_bigrams_with_metrics(
                texts=filtered['cleaned_text'],
                ratings=filtered['rating'],
                original_texts=filtered['original_text'],
                top_n=20,
                adj_priority=adj_priority_val,
                min_df=min_df_val
            )
        
        # ê¸ì •/ë¶€ì • í‚¤ì›Œë“œ ë¶„ë¦¬ (ê°€ë³€ ì„ê³„ê°’ ê¸°ì¤€)
        diverging_keywords = get_diverging_keywords(
            keywords_analysis, 
            top_n=8, 
            threshold=impact_threshold_val
        )
    except Exception as e:
        print(f"í‚¤ì›Œë“œ ë¶„ì„ ì˜¤ë¥˜: {e}")
    
    # =========================================================
    # =========================================================
    # 4. Diverging Bar Chart (ê°ì„± ì˜í–¥ë„ ì‹œê°í™”)
    # =========================================================
    
    # ë¶€ì • í‚¤ì›Œë“œ (Impact Score < 0)
    neg_keywords = diverging_keywords["negative"]
    pos_keywords = diverging_keywords["positive"]
    
    # Diverging Bar Chart ìƒì„± (xì¶• ê¸°ì¤€ 0)
    fig_diverging = go.Figure()
    
    # ë¶€ì • ì˜í–¥ í‚¤ì›Œë“œ (ì™¼ìª½, ë¹¨ê°„ìƒ‰)
    if neg_keywords:
        fig_diverging.add_trace(go.Bar(
            y=[k["keyword"] for k in neg_keywords],
            x=[k["impact_score"] for k in neg_keywords],
            orientation='h',
            name='ë¶€ì • ì˜í–¥',
            marker_color='#ef4444',
            text=[f'SI: {k["satisfaction_index"]}' for k in neg_keywords],
            textposition='inside',
            hovertemplate='<b>%{y}</b><br>ê°ì„± ì˜í–¥ë„: %{x}<br>ë§Œì¡±ë„ ì§€ìˆ˜: %{text}<extra></extra>'
        ))
    
    # ê¸ì • ì˜í–¥ í‚¤ì›Œë“œ (ì˜¤ë¥¸ìª½, ë…¹ìƒ‰)
    if pos_keywords:
        fig_diverging.add_trace(go.Bar(
            y=[k["keyword"] for k in pos_keywords],
            x=[k["impact_score"] for k in pos_keywords],
            orientation='h',
            name='ê¸ì • ì˜í–¥',
            marker_color='#22c55e',
            text=[f'SI: {k["satisfaction_index"]}' for k in pos_keywords],
            textposition='inside',
            hovertemplate='<b>%{y}</b><br>ê°ì„± ì˜í–¥ë„: %{x}<br>ë§Œì¡±ë„ ì§€ìˆ˜: %{text}<extra></extra>'
        ))
    
    fig_diverging.update_layout(
        title="í‚¤ì›Œë“œë³„ ê°ì„± ì˜í–¥ë„ (Impact Score)",
        xaxis_title="ê°ì„± ì˜í–¥ë„ (Impact Score: 0 = í‰ê· )",
        yaxis_title="í‚¤ì›Œë“œ (Bigram)",
        template="plotly_white",
        height=500,
        showlegend=True,
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        xaxis=dict(zeroline=True, zerolinewidth=2, zerolinecolor='#64748b'),
        barmode='relative'
    )
    
    # =========================================================
    # 5. Satisfaction Index Chart (ë§Œì¡±ë„ í™•ë¥  ì§€ìˆ˜ ì‹œê°í™”)
    # =========================================================
    
    # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ
    top_keywords_for_si = sorted(keywords_analysis, key=lambda x: -x["mention_count"])[:10]
    
    fig_positivity = go.Figure()
    if top_keywords_for_si:
        fig_positivity.add_trace(go.Bar(
            x=[k["keyword"] for k in top_keywords_for_si],
            y=[k["satisfaction_index"] for k in top_keywords_for_si],
            marker_color=[
                '#22c55e' if k["satisfaction_index"] >= 1.2 else '#f59e0b' if k["satisfaction_index"] >= 0.8 else '#ef4444'
                for k in top_keywords_for_si
            ],
            text=[f'{k["satisfaction_index"]}' for k in top_keywords_for_si],
            textposition='outside',
            hovertemplate='<b>%{x}</b><br>ë§Œì¡±ë„ ì§€ìˆ˜: %{y}<br>ì–¸ê¸‰ íšŸìˆ˜: %{customdata}<extra></extra>',
            customdata=[k["mention_count"] for k in top_keywords_for_si]
        ))
        
        # ê¸°ì¤€ì„  1.0 ì¶”ê°€
        fig_positivity.add_shape(
            type="line",
            x0=-0.5, y0=1.0, x1=len(top_keywords_for_si)-0.5, y1=1.0,
            line=dict(color="Red", width=2, dash="dash"),
        )
    
    fig_positivity.update_layout(
        title="í‚¤ì›Œë“œë³„ ë§Œì¡±ë„ í™•ë¥  ì§€ìˆ˜ (Satisfaction Index)",
        xaxis_title="í‚¤ì›Œë“œ (Bigram)",
        yaxis_title="Index (ê¸°ì¤€ 1.0)",
        template="plotly_white",
        height=400,
        yaxis=dict(rangemode='tozero') 
    )

    # =========================================================
    # 6. Advanced Consumer Experience Metrics
    # =========================================================

    # NSS (Net Sentiment Score) ê³„ì‚°
    pos_count = filtered[filtered['sentiment_score'] >= 0.75].shape[0]
    neg_count = filtered[filtered['sentiment_score'] <= 0.25].shape[0]
    nss_score = ((pos_count - neg_count) / total_count * 100) if total_count > 0 else 0
    
    # CAS (Customer Advocacy Score)
    advocates = filtered[
        (filtered['repurchase_intent_hybrid'] == True) & 
        (filtered['recommendation_intent_hybrid'] == True)
    ].shape[0]
    cas_score = (advocates / total_count) if total_count > 0 else 0
    
    # NSS ê²Œì´ì§€ ì°¨íŠ¸
    fig_nss = go.Figure(go.Indicator(
        mode = "gauge+number",
        value = nss_score,
        title = {'text': "NSS (ìˆœ ì •ì„œ ì ìˆ˜)"},
        gauge = {
            'axis': {'range': [-100, 100]},
            'bar': {'color': "darkblue"},
            'steps' : [
                {'range': [-100, -30], 'color': "#ff4d4f"},
                {'range': [-30, 30], 'color': "#faad14"},
                {'range': [30, 100], 'color': "#52c41a"}
            ],
            'threshold' : {'line': {'color': "black", 'width': 4}, 'thickness': 0.75, 'value': nss_score}
        }
    ))
    fig_nss.update_layout(height=300, margin=dict(l=20, r=20, t=50, b=20))

    # ASINë³„ NSS vs CAS ì‚°ì ë„
    # ASINë³„ NSS vs CAS ì‚°ì ë„ (Global Comparative Analysis)
    # ë©”ëª¨ë¦¬ ë¬¸ì œë¡œ ì „ì²´ ë°ì´í„°(df_consumer) ë¡œë”©ì„ ì•ˆí•˜ë¯€ë¡œ, 
    # ë¹„êµ ë¶„ì„ ëŒ€ì‹  í˜„ì¬ ê²€ìƒ‰ëœ ìƒí’ˆë“¤ì˜ ë¶„í¬ë§Œ ë³´ì—¬ì£¼ê±°ë‚˜, DB ì§‘ê³„ê°€ í•„ìš”í•¨.
    # ì—¬ê¸°ì„œëŠ” ê²€ìƒ‰ëœ ë°ì´í„°(filtered) ë‚´ì˜ ASINë“¤ë§Œ ë¹„êµí•˜ëŠ” ê²ƒìœ¼ë¡œ ì¶•ì†Œ.
    try:
        asin_stats = filtered.groupby('asin').agg(
            total=('sentiment_score', 'count'),
            pos_count=('sentiment_score', lambda x: (x >= 0.75).sum()),
            neg_count=('sentiment_score', lambda x: (x <= 0.25).sum())
        ).reset_index()
        
        cas_counts = filtered[
            (filtered['repurchase_intent_hybrid'] == True) & 
            (filtered['recommendation_intent_hybrid'] == True)
        ].groupby('asin').size().reset_index(name='adv_count')
        
        asin_stats = pd.merge(asin_stats, cas_counts, on='asin', how='left').fillna(0)
        asin_stats['nss'] = (asin_stats['pos_count'] - asin_stats['neg_count']) / asin_stats['total'] * 100
        asin_stats['cas'] = asin_stats['adv_count'] / asin_stats['total']
    except Exception as e:
        print(f"ASIN Stats Error: {e}")
        asin_stats = pd.DataFrame(columns=['asin', 'nss', 'cas', 'total']) # Empty fallback
    
    current_asins = filtered['asin'].unique()
    fig_scatter_nss = go.Figure()
    fig_scatter_nss.add_trace(go.Scatter(
        x=asin_stats['nss'], y=asin_stats['cas'],
        mode='markers',
        marker=dict(color='lightgray', size=8, opacity=0.5),
        name='íƒ€ì‚¬ ì œí’ˆ'
    ))
    curr_stats = asin_stats[asin_stats['asin'].isin(current_asins)]
    fig_scatter_nss.add_trace(go.Scatter(
        x=curr_stats['nss'], y=curr_stats['cas'],
        mode='markers',
        marker=dict(color='red', size=12, symbol='star'),
        name='í˜„ì¬ ë¶„ì„ ì œí’ˆ'
    ))
    fig_scatter_nss.update_layout(
        title="ë¸Œëœë“œ í¬ì§€ì…”ë‹ (NSS vs CAS)",
        xaxis_title="NSS (ìˆœ ì •ì„œ ì ìˆ˜)",
        yaxis_title="CAS (ê³ ê° ì˜¹í˜¸ ì ìˆ˜)",
        template="plotly_white",
        height=400
    )

    # PQI (Product Quality Index)
    quality_exploded = filtered.explode('quality_issues_semantic')
    quality_issues_count = quality_exploded['quality_issues_semantic'].dropna().value_counts()
    total_issues = quality_issues_count.sum()
    pqi_score = max(0, 100 - (total_issues / total_count * 20)) if total_count > 0 else 100
    
    fig_treemap = go.Figure()
    if not quality_issues_count.empty:
        fig_treemap = px.treemap(
            names=quality_issues_count.index,
            parents=["Quality Issues"] * len(quality_issues_count),
            values=quality_issues_count.values,
            title="ì£¼ìš” í’ˆì§ˆ ë¶ˆë§Œ (Quality Issues)"
        )

    # LFI (Logistics Friction Index)
    lfi_keywords = ['dent', 'leak', 'broken', 'damage', 'crush', 'open']
    lfi_count = 0
    for col in ['delivery_issues_semantic', 'packaging_keywords']:
        if col in filtered.columns:
            exploded = filtered.explode(col)
            mask = exploded[col].astype(str).str.contains('|'.join(lfi_keywords), case=False, na=False)
            lfi_count += mask.sum()
    lfi_rate = (lfi_count / total_count * 100) if total_count > 0 else 0
    
    # SPI (Sensory Performance Index)
    spi_score = (filtered[filtered['sensory_conflict'] == False].shape[0] / total_count * 100) if total_count > 0 else 0
    
    texture_exploded = filtered.explode('texture_terms')
    texture_sentiment = texture_exploded.groupby('texture_terms')['sentiment_score'].mean().sort_values(ascending=False).head(8)
    
    fig_radar = go.Figure()
    if not texture_sentiment.empty:
        categories = texture_sentiment.index.tolist()
        values = texture_sentiment.values.tolist()
        fig_radar = go.Figure(data=go.Scatterpolar(
            r=values + [values[0]],
            theta=categories + [categories[0]],
            fill='toself',
            name='Texture Sentiment'
        ))
        fig_radar.update_layout(
            polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
            title="ì‹ê°ë³„ ì„ í˜¸ë„ (Textural Preference)",
            height=400
        )

    # Value & Price
    value_score = filtered['value_perception_hybrid'].mean()
    price_sensitive_ratio = filtered['price_sensitive'].mean() if 'price_sensitive' in filtered.columns else 0
    
    marketing_stats = filtered.groupby('asin').agg(
        avg_value=('value_perception_hybrid', 'mean'),
        price_sens=('price_sensitive', 'mean')
    ).reset_index()
    if 'title' in filtered.columns:
        titles = filtered.groupby('asin')['title'].first().reset_index()
        marketing_stats = pd.merge(marketing_stats, titles, on='asin', how='left')
    else:
        marketing_stats['title'] = marketing_stats['asin']
    
    fig_marketing = go.Figure()
    fig_marketing.add_trace(go.Scatter(
        x=marketing_stats['price_sens'], y=marketing_stats['avg_value'],
        mode='markers', text=marketing_stats['title'],
        marker=dict(color='#8884d8', opacity=0.5), name='íƒ€ì‚¬ ì œí’ˆ'
    ))
    curr_mk = marketing_stats[marketing_stats['asin'].isin(current_asins)]
    fig_marketing.add_trace(go.Scatter(
        x=curr_mk['price_sens'], y=curr_mk['avg_value'],
        mode='markers', text=curr_mk['title'],
        marker=dict(color='#ff7300', size=15, symbol='diamond'), name='í˜„ì¬ ì œí’ˆ'
    ))
    fig_marketing.add_hline(y=0, line_dash="dash", line_color="gray")
    fig_marketing.add_vline(x=0.5, line_dash="dash", line_color="gray")
    fig_marketing.update_layout(title="ê°€ì¹˜-ê°€ê²© í¬ì§€ì…”ë‹ ë§µ", xaxis_title="ê°€ê²© ë¯¼ê°ë„", yaxis_title="ê°€ì¹˜ ì¸ì‹", template="plotly_white")

    return {
        "has_data": True,
        "search_term": item_name if item_name else item_id,
        "metrics": {
            "nss": round(nss_score, 2),
            "cas": round(cas_score, 2),
            "pqi": round(pqi_score, 2),
            "lfi": round(lfi_rate, 2),
            "spi": round(spi_score, 2),
            "value_score": round(value_score, 2),
            "price_sensitivity": round(price_sensitive_ratio, 2)
        },
        "keywords_analysis": keywords_analysis[:50],
        "diverging_summary": {
            "negative_keywords": [{"keyword": k["keyword"], "impact_score": k["impact_score"], "satisfaction_index": k.get("satisfaction_index", 0)} for k in neg_keywords],
            "positive_keywords": [{"keyword": k["keyword"], "impact_score": k["impact_score"], "satisfaction_index": k.get("satisfaction_index", 0)} for k in pos_keywords]
        },
        "charts": {
            "impact_diverging_bar": json.loads(fig_diverging.to_json()),
            "positivity_bar": json.loads(fig_positivity.to_json()),
            "nss_gauge": json.loads(fig_nss.to_json()),
            "nss_cas_scatter": json.loads(fig_scatter_nss.to_json()),
            "quality_treemap": json.loads(fig_treemap.to_json()),
            "sensory_radar": json.loads(fig_radar.to_json()),
            "marketing_matrix": json.loads(fig_marketing.to_json())
        }
    }

@app.get("/dashboard")
async def dashboard():
    if df is None or df.empty:
        return {"has_data": False}
        
    try:
        # 1. Top 5 êµ­ê°€ ìˆ˜ì¶œ ì¶”ì„¸ (Line)
        # êµ­ê°€ë³„, ì›”ë³„ í•©ì‚°
        country_trend = df.groupby(['period_str', 'country_name'])['export_value'].sum().reset_index()
        # ì´ ìˆ˜ì¶œì•¡ ê¸°ì¤€ Top 5 êµ­ê°€ ì„ ì •
        top_countries = df.groupby('country_name')['export_value'].sum().nlargest(5).index.tolist()
        country_trend_top = country_trend[country_trend['country_name'].isin(top_countries)]
        
        fig1 = px.line(country_trend_top, x='period_str', y='export_value', color='country_name',
                       title="1. Top 5 êµ­ê°€ ìˆ˜ì¶œ ì¶”ì„¸ (Market Trend)")
        fig1.update_layout(template="plotly_white", margin=dict(l=20, r=20, t=40, b=20), xaxis_title="ê¸°ê°„", yaxis_title="ìˆ˜ì¶œì•¡ ($)")

        # 2. Top 5 í’ˆëª© ìˆ˜ì¶œ ì¶”ì„¸ (Line)
        item_trend = df.groupby(['period_str', 'item_name'])['export_value'].sum().reset_index()
        top_items = df.groupby('item_name')['export_value'].sum().nlargest(5).index.tolist()
        item_trend_top = item_trend[item_trend['item_name'].isin(top_items)]
        
        # UI ì´ë¦„ìœ¼ë¡œ ë§¤í•‘
        item_trend_top['ui_name'] = item_trend_top['item_name'].apply(lambda x: CSV_TO_UI_ITEM_MAPPING.get(x, x))
        
        fig2 = px.line(item_trend_top, x='period_str', y='export_value', color='ui_name',
                       title="2. Top 5 í’ˆëª© ìˆ˜ì¶œ ì¶”ì„¸ (Product Lifecycle)")
        fig2.update_layout(template="plotly_white", margin=dict(l=20, r=20, t=40, b=20), xaxis_title="ê¸°ê°„", yaxis_title="ìˆ˜ì¶œì•¡ ($)")

        # 3. êµ­ê°€ë³„ í‰ê·  ë‹¨ê°€ ë¹„êµ (Bar - Profitability)
        # ë‹¨ê°€ = ì´ ìˆ˜ì¶œì•¡ / ì´ ì¤‘ëŸ‰ (ì¤‘ëŸ‰ ì—†ìœ¼ë©´ unit_price í‰ê·  ëŒ€ìš©)
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ unit_priceì˜ í‰ê· ì„ êµ­ê°€ë³„ë¡œ ë¹„êµ
        profitability = df.groupby('country_name')['unit_price'].mean().sort_values(ascending=False).reset_index()
        
        fig3 = px.bar(profitability, x='country_name', y='unit_price', color='unit_price',
                      title="3. êµ­ê°€ë³„ í‰ê·  ë‹¨ê°€ (Profitability Check)", color_continuous_scale='Viridis')
        fig3.update_layout(template="plotly_white", margin=dict(l=20, r=20, t=40, b=20), xaxis_title="êµ­ê°€", yaxis_title="í‰ê·  ë‹¨ê°€ ($/kg)")

        # 4. ì‹œì¥ í¬ì§€ì…”ë‹ ë§µ (Scatter - Volume vs Value)
        # êµ­ê°€ë³„ ì´ ìˆ˜ì¶œì•¡(Value) vs ì´ ì¤‘ëŸ‰(Volume)
        positioning = df.groupby('country_name').agg({
            'export_value': 'sum',
            'export_weight': 'sum'
        }).reset_index()
        
        fig4 = px.scatter(positioning, x='export_weight', y='export_value', text='country_name',
                          size='export_value', color='country_name',
                          title="4. ì‹œì¥ í¬ì§€ì…”ë‹ (Volume vs Value)")
        fig4.update_traces(textposition='top center')
        fig4.update_layout(template="plotly_white", margin=dict(l=20, r=20, t=40, b=20), 
                           xaxis_title="ì´ ë¬¼ëŸ‰ (Volume)", yaxis_title="ì´ ê¸ˆì•¡ (Value)")

        # 5. í’ˆëª©ë³„ ì›”ë³„ ê³„ì ˆì„± (Heatmap)
        # ì›”(Month) ì¶”ì¶œ
        df['month'] = df['period_str'].apply(lambda x: x.split('-')[1] if '-' in str(x) else '00')
        seasonality = df[df['item_name'].isin(top_items)].groupby(['item_name', 'month'])['export_value'].sum().reset_index()
        
        # UI ì´ë¦„ ë§¤í•‘
        seasonality['ui_name'] = seasonality['item_name'].apply(lambda x: CSV_TO_UI_ITEM_MAPPING.get(x, x))
        
        # Pivot for Heatmap: Index=Item, Columns=Month, Values=ExportValue
        heatmap_data = seasonality.pivot(index='ui_name', columns='month', values='export_value').fillna(0)
        # ì›” ìˆœì„œ ì •ë ¬
        sorted_months = sorted(heatmap_data.columns)
        heatmap_data = heatmap_data[sorted_months]
        
        fig5 = px.imshow(heatmap_data, labels=dict(x="ì›” (Month)", y="í’ˆëª©", color="ìˆ˜ì¶œì•¡"),
                         title="5. ê³„ì ˆì„± ë¶„ì„ (Seasonality Heatmap)", aspect="auto", color_continuous_scale='OrRd')
        fig5.update_layout(template="plotly_white", margin=dict(l=20, r=20, t=40, b=20))

        return {
            "has_data": True,
            "charts": {
                "top_countries": json.loads(fig1.to_json()),
                "top_items": json.loads(fig2.to_json()),
                "profitability": json.loads(fig3.to_json()),
                "positioning": json.loads(fig4.to_json()),
                "seasonality": json.loads(fig5.to_json())
            }
        }
    except Exception as e:
        print(f"Dashboard Error: {e}")
        return {"has_data": False, "error": str(e)}

if __name__ == "__main__":
    import uvicorn
    for route in app.routes:
        print(f"Route: {route.path} {route.name}")
    uvicorn.run(app, host="0.0.0.0", port=8000)
